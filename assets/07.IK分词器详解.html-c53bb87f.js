import{_ as t,V as e,W as i,Y as a,Z as n,$ as o,X as c,F as p}from"./framework-a569e214.js";const l="/img/elasticsearch/image-20210319094829484.png",g="/img/elasticsearch/image-20210319094955038.png",r="/img/elasticsearch/image-20210319095718159.png",u="/img/elasticsearch/image-20210319095620782.png",m="/img/elasticsearch/image-20210319100423902.png",d="/img/elasticsearch/image-20210319100520276.png",k="/img/elasticsearch/image-20210319101709540.png",_="/img/elasticsearch/image-20210319101731204.png",f="/img/elasticsearch/image-20210319101320841.png",h="/img/elasticsearch/image-20210319102211372.png",b="/img/elasticsearch/image-20210319101010899.png",v="/img/elasticsearch/image-20210319101031773.png",x={},y=a("h2",{id:"什么是ik分词器",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#什么是ik分词器","aria-hidden":"true"},"#"),n(" 什么是IK分词器")],-1),q=a("p",null,'分词：即把一段中文或者别的划分成一个个的关键字。我们在搜索时候会把自己的信息进行分词，会把数据库中或者索引库中的数据进行分词，然后进行一个匹配操作。默认的中文分词是将每个字看成一个词，比如“我爱狂神”会被为“我"爱”“狂"神” 。这显然是不符合要求的，所以我们需要安装中文分词器ik来解决这个问题。',-1),z=a("blockquote",null,[a("p",null,[n("IK提供了两个分词算法："),a("strong",null,"ik_smart"),n("和"),a("strong",null,"ik_max_word"),n(" ，其中ik_smart为最少切分， ik_max _word为最细粒度划分。")])],-1),I=a("h2",{id:"安装",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#安装","aria-hidden":"true"},"#"),n(" 安装")],-1),E={href:"https://github.com/medcl/elasticsearch-analysis-ik",target:"_blank",rel:"noopener noreferrer"},K=c('<p>下载完毕之后，解压放入ElasticSearch中的插件目录即可。</p><figure><img src="'+l+'" alt="image-20210319094829484" tabindex="0" loading="lazy"><figcaption>image-20210319094829484</figcaption></figure><figure><img src="'+g+'" alt="image-20210319094955038" tabindex="0" loading="lazy"><figcaption>image-20210319094955038</figcaption></figure><blockquote><p>重启ES观察，可以看到IK分词器被加载了。</p></blockquote><figure><img src="'+r+'" alt="image-20210319095718159" tabindex="0" loading="lazy"><figcaption>image-20210319095718159</figcaption></figure><p>使用<code>elasticsearch-plugin list</code>命令查看加载的插件：</p><figure><img src="'+u+'" alt="image-20210319095620782" tabindex="0" loading="lazy"><figcaption>image-20210319095620782</figcaption></figure><h2 id="使用kibana进行测试" tabindex="-1"><a class="header-anchor" href="#使用kibana进行测试" aria-hidden="true">#</a> 使用Kibana进行测试</h2><blockquote><p>ik_smart 最少切分</p></blockquote><figure><img src="'+m+'" alt="image-20210319100423902" tabindex="0" loading="lazy"><figcaption>image-20210319100423902</figcaption></figure><blockquote><p>ik_max_word 最细粒度划分。穷尽词库的可能。</p></blockquote><figure><img src="'+d+'" alt="image-20210319100520276" tabindex="0" loading="lazy"><figcaption>image-20210319100520276</figcaption></figure><h2 id="字典" tabindex="-1"><a class="header-anchor" href="#字典" aria-hidden="true">#</a> 字典</h2><figure><img src="'+k+'" alt="image-20210319101709540" tabindex="0" loading="lazy"><figcaption>image-20210319101709540</figcaption></figure><figure><img src="'+_+'" alt="image-20210319101731204" tabindex="0" loading="lazy"><figcaption>image-20210319101731204</figcaption></figure><p>发现问题：狂神说被拆开了。自己需要的词，需要我们自己加到我们的分词器的字典中。</p><blockquote><p>配置自己的字典</p></blockquote><figure><img src="'+f+`" alt="image-20210319101320841" tabindex="0" loading="lazy"><figcaption>image-20210319101320841</figcaption></figure><p><code>IKAnalyzer.cfg.xml</code></p><div class="language-xml line-numbers-mode" data-ext="xml"><pre class="language-xml"><code><span class="token prolog">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token doctype-tag">DOCTYPE</span> <span class="token name">properties</span> <span class="token name">SYSTEM</span> <span class="token string">&quot;http://java.sun.com/dtd/properties.dtd&quot;</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>comment</span><span class="token punctuation">&gt;</span></span>IK Analyzer 扩展配置<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>comment</span><span class="token punctuation">&gt;</span></span>
	<span class="token comment">&lt;!--用户可以在这里配置自己的扩展字典 --&gt;</span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>entry</span> <span class="token attr-name">key</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">&quot;</span>ext_dict<span class="token punctuation">&quot;</span></span><span class="token punctuation">&gt;</span></span>kuang.dic<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>entry</span><span class="token punctuation">&gt;</span></span>
	 <span class="token comment">&lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;</span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>entry</span> <span class="token attr-name">key</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">&quot;</span>ext_stopwords<span class="token punctuation">&quot;</span></span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>entry</span><span class="token punctuation">&gt;</span></span>
	<span class="token comment">&lt;!--用户可以在这里配置远程扩展字典 --&gt;</span>
	<span class="token comment">&lt;!-- &lt;entry key=&quot;remote_ext_dict&quot;&gt;words_location&lt;/entry&gt; --&gt;</span>
	<span class="token comment">&lt;!--用户可以在这里配置远程扩展停止词字典--&gt;</span>
	<span class="token comment">&lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">&gt;</span></span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>kuang.dic</code></p><div class="language-java line-numbers-mode" data-ext="java"><pre class="language-java"><code>狂神说
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>重新启动ES：</p><figure><img src="`+h+'" alt="image-20210319102211372" tabindex="0" loading="lazy"><figcaption>image-20210319102211372</figcaption></figure><p>再次测试：</p><figure><img src="'+b+'" alt="image-20210319101010899" tabindex="0" loading="lazy"><figcaption>image-20210319101010899</figcaption></figure><figure><img src="'+v+'" alt="image-20210319101031773" tabindex="0" loading="lazy"><figcaption>image-20210319101031773</figcaption></figure><blockquote><p>以后的话，我们需要自己配置分词就在自己定义的dic文件中进行配置即可。</p></blockquote>',28);function w(S,V){const s=p("ExternalLinkIcon");return e(),i("div",null,[y,q,z,I,a("p",null,[n("下载地址："),a("a",E,[n("https://github.com/medcl/elasticsearch-analysis-ik"),o(s)])]),K])}const N=t(x,[["render",w],["__file","07.IK分词器详解.html.vue"]]);export{N as default};
